{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "print('kernel running (use gpu3)')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kernel running (use gpu3)\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.utils import np_utils\n",
        "import tensorflow.keras as keras\n",
        "import random\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment, Run, Workspace\n",
        "import azureml.core\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)\n",
        "\n",
        "ws = Workspace.from_config()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SDK version: 1.20.0\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Buffy scripts"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Just the first 20 shows for speed\n",
        "allscripts = os.listdir(\"./BuffyScripts\")\n",
        "raw_text_all = \"\"\n",
        "i=-1\n",
        "for script in allscripts:\n",
        "    i = i+1\n",
        "    # load ascii text and covert to lowercase\n",
        "    if (i < 20):\n",
        "        filename = script\n",
        "        raw_text = open(\"BuffyScripts/\" + filename, 'r', encoding='utf-8').read()\n",
        "        raw_text = raw_text.lower()\n",
        "        raw_text_all = raw_text_all + raw_text\n",
        "\n",
        "text = raw_text_all\n",
        "\n",
        "print('text length', len(text))\n",
        "chars = sorted(list(set(text))) # getting all unique chars\n",
        "print('total chars: ', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text length 885808\n",
            "total chars:  58\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Identify all the characters we have in the files\n",
        "chars"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "['\\n',\n ' ',\n '!',\n '\"',\n '#',\n '%',\n '&',\n \"'\",\n '(',\n ')',\n '*',\n ',',\n '-',\n '.',\n '/',\n '0',\n '1',\n '2',\n '3',\n '4',\n '5',\n '6',\n '7',\n '8',\n '9',\n ':',\n '?',\n 'a',\n 'b',\n 'c',\n 'd',\n 'e',\n 'f',\n 'g',\n 'h',\n 'i',\n 'j',\n 'k',\n 'l',\n 'm',\n 'n',\n 'o',\n 'p',\n 'q',\n 'r',\n 's',\n 't',\n 'u',\n 'v',\n 'w',\n 'x',\n 'y',\n 'z',\n '~',\n '©',\n '»',\n '¼',\n 'ã']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#split our data up into sections with a length of 40 character\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "#"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Display of sections\n",
        "print(\"length: \" + str(len(sentences[360])))\n",
        "print(sentences[360])\n",
        "print(\"   \" + sentences[361])\n",
        "print(\"   \" + \"   \" + sentences[362])\n",
        "print(\"   \" + \"   \" + \"   \" + sentences[363])\n",
        "\n",
        "s = 1847\n",
        "print(sentences[s])\n",
        "print(\"   \" + sentences[s+1])\n",
        "print(\"   \" + \"   \" + sentences[s+2])\n",
        "print(\"   \" + \"   \" + \"   \" + sentences[s+3])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length: 40\n",
            "he gym it's so cool. you can see the who\n",
            "   gym it's so cool. you can see the whole \n",
            "       it's so cool. you can see the whole tow\n",
            "         's so cool. you can see the whole town. \n",
            "\n",
            "mr. flutie: buffy, don't worry. any oth\n",
            "   . flutie: buffy, don't worry. any other \n",
            "      lutie: buffy, don't worry. any other sch\n",
            "         ie: buffy, don't worry. any other school\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#model = Sequential()\n",
        "#model.add(LSTM(256, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(LSTM(256))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(Dense(y.shape[1], activation='softmax'))\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard DNN - good for classification - but context is lossed every time the input changes"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": [
        "In this example run digit recognition on the following with a DNN"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DNN has no context on if the last character should be a y or T"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ------------------------------- B u f f T -------------------------------"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent DNN - output is fed through to next input"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BPTT (backpropagation through time)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  ------------------------------- B u f f y -------------------------------"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDNN can suffer from the vanishing gradient problem"
      ],
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": [
        "Feedback becomes so small as to not affect the learning"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Long Short Term Memory"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input gate<br>\n",
        "Forget Gate<br>\n",
        "Output gate<br>\n",
        "\n",
        "![image-2.png](attachment:image-2.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a simple LSTM DNN \n",
        "model = Sequential()\n",
        "#model.add(LSTM(128, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
        "model.add(LSTM(64, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(64))\n",
        "#model.add(LSTM(128))\n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 40, 64)            31488     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 40, 64)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 58)                3770      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 58)                0         \n",
            "=================================================================\n",
            "Total params: 68,282\n",
            "Trainable params: 68,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install pydot\n",
        "#! pip install graphviz "
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png',  show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=True)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "image/png": "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\n",
            "text/plain": "<IPython.core.display.Image object>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 1436937308381062860\n",
            "]\n"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# choose a name for experiment\n",
        "epochs = 515\n",
        "\n",
        "experiment=Experiment(ws, 'buffy')\n",
        "\n",
        "run = experiment.start_logging(snapshot_directory=None)\n",
        "\n",
        "history = model.fit(x, y, batch_size=1028, epochs=epochs)\n",
        "\n",
        "model.save('predmodel.h5')\n",
        "run.upload_file(\"outputs/predmodel.h5\", \"predmodel.h5\")\n",
        "#run.register_model(model_name='model' + str(epochs), model_path='outputs/predmodel')\n",
        "\n",
        "run.log('epochs', epochs)\n",
        "run.log('LSTM', 64)\n",
        "run.log('Min Loss', min(min(history.history.values())))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "run.log_image(\"loss\", path=None, plot=plt)\n",
        "plt.show()\n",
        "\n",
        "run.complete()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 2.3006\n",
            "Epoch 2/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.7085\n",
            "Epoch 3/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.5711\n",
            "Epoch 4/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.5026\n",
            "Epoch 5/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.4586\n",
            "Epoch 6/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.4288\n",
            "Epoch 7/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.4044\n",
            "Epoch 8/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.3873\n",
            "Epoch 9/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.3747\n",
            "Epoch 10/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.3616\n",
            "Epoch 11/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.3515\n",
            "Epoch 12/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.3431\n",
            "Epoch 13/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.3359\n",
            "Epoch 14/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.3285\n",
            "Epoch 15/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.3230\n",
            "Epoch 16/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.3175\n",
            "Epoch 17/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.3132\n",
            "Epoch 18/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.3087\n",
            "Epoch 19/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.3049\n",
            "Epoch 20/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.3010\n",
            "Epoch 21/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2974\n",
            "Epoch 22/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2951\n",
            "Epoch 23/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2914\n",
            "Epoch 24/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2908\n",
            "Epoch 25/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2872\n",
            "Epoch 26/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2847\n",
            "Epoch 27/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2817\n",
            "Epoch 28/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2808\n",
            "Epoch 29/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2781\n",
            "Epoch 30/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2774\n",
            "Epoch 31/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2736\n",
            "Epoch 32/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2732\n",
            "Epoch 33/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2707\n",
            "Epoch 34/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2705\n",
            "Epoch 35/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2676\n",
            "Epoch 36/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2650\n",
            "Epoch 37/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2657\n",
            "Epoch 38/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2651\n",
            "Epoch 39/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2631\n",
            "Epoch 40/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2612\n",
            "Epoch 41/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2601\n",
            "Epoch 42/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2585\n",
            "Epoch 43/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2572\n",
            "Epoch 44/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2576\n",
            "Epoch 45/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2546\n",
            "Epoch 46/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2548\n",
            "Epoch 47/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2552\n",
            "Epoch 48/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2528\n",
            "Epoch 49/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2513\n",
            "Epoch 50/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2507\n",
            "Epoch 51/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2504\n",
            "Epoch 52/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2495\n",
            "Epoch 53/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2485\n",
            "Epoch 54/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2466\n",
            "Epoch 55/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2473\n",
            "Epoch 56/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2456\n",
            "Epoch 57/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2444\n",
            "Epoch 58/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2440\n",
            "Epoch 59/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2452\n",
            "Epoch 60/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2424\n",
            "Epoch 61/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2423\n",
            "Epoch 62/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2413\n",
            "Epoch 63/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2398\n",
            "Epoch 64/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2402\n",
            "Epoch 65/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2389\n",
            "Epoch 66/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2381\n",
            "Epoch 67/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2383\n",
            "Epoch 68/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2388\n",
            "Epoch 69/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2385\n",
            "Epoch 70/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2358\n",
            "Epoch 71/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2353\n",
            "Epoch 72/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2371\n",
            "Epoch 73/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2349\n",
            "Epoch 74/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2348\n",
            "Epoch 75/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2354\n",
            "Epoch 76/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2348\n",
            "Epoch 77/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2340\n",
            "Epoch 78/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2331\n",
            "Epoch 79/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2326\n",
            "Epoch 80/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2310\n",
            "Epoch 81/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2318\n",
            "Epoch 82/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2317\n",
            "Epoch 83/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2292\n",
            "Epoch 84/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2282\n",
            "Epoch 85/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2296\n",
            "Epoch 86/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2307\n",
            "Epoch 87/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2280\n",
            "Epoch 88/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2275\n",
            "Epoch 89/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2274\n",
            "Epoch 90/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2265\n",
            "Epoch 91/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2274\n",
            "Epoch 92/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2262\n",
            "Epoch 93/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2255\n",
            "Epoch 94/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2257\n",
            "Epoch 95/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2270\n",
            "Epoch 96/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2250\n",
            "Epoch 97/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2247\n",
            "Epoch 98/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2253\n",
            "Epoch 99/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2229\n",
            "Epoch 100/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2235\n",
            "Epoch 101/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2253\n",
            "Epoch 102/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2236\n",
            "Epoch 103/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2227\n",
            "Epoch 104/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2222\n",
            "Epoch 105/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2228\n",
            "Epoch 106/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2210\n",
            "Epoch 107/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2220\n",
            "Epoch 108/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2217\n",
            "Epoch 109/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2213\n",
            "Epoch 110/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2211\n",
            "Epoch 111/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2227\n",
            "Epoch 112/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2211\n",
            "Epoch 113/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2199\n",
            "Epoch 114/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2190\n",
            "Epoch 115/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2190\n",
            "Epoch 116/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2178\n",
            "Epoch 117/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2191\n",
            "Epoch 118/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2186\n",
            "Epoch 119/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2186\n",
            "Epoch 120/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2189\n",
            "Epoch 121/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2177\n",
            "Epoch 122/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2178\n",
            "Epoch 123/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2172\n",
            "Epoch 124/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2157\n",
            "Epoch 125/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2161\n",
            "Epoch 126/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2162\n",
            "Epoch 127/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2173\n",
            "Epoch 128/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2158\n",
            "Epoch 129/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2165\n",
            "Epoch 130/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2162\n",
            "Epoch 131/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2161\n",
            "Epoch 132/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2152\n",
            "Epoch 133/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2155\n",
            "Epoch 134/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2152\n",
            "Epoch 135/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2141\n",
            "Epoch 136/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2145\n",
            "Epoch 137/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2142\n",
            "Epoch 138/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2138\n",
            "Epoch 139/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2145\n",
            "Epoch 140/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2125\n",
            "Epoch 141/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2142\n",
            "Epoch 142/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2138\n",
            "Epoch 143/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2120\n",
            "Epoch 144/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2109\n",
            "Epoch 145/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2125\n",
            "Epoch 146/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2128\n",
            "Epoch 147/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2109\n",
            "Epoch 148/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2126\n",
            "Epoch 149/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2099\n",
            "Epoch 150/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2119\n",
            "Epoch 151/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2114\n",
            "Epoch 152/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2099\n",
            "Epoch 153/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2117\n",
            "Epoch 154/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2111\n",
            "Epoch 155/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2097\n",
            "Epoch 156/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2118\n",
            "Epoch 157/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2098\n",
            "Epoch 158/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2093\n",
            "Epoch 159/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2103\n",
            "Epoch 160/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2088\n",
            "Epoch 161/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2083\n",
            "Epoch 162/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2086\n",
            "Epoch 163/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.2099\n",
            "Epoch 164/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2086\n",
            "Epoch 165/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2096\n",
            "Epoch 166/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2093\n",
            "Epoch 167/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2097\n",
            "Epoch 168/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2074\n",
            "Epoch 169/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2087\n",
            "Epoch 170/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2088\n",
            "Epoch 171/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2079\n",
            "Epoch 172/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2068\n",
            "Epoch 173/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2068\n",
            "Epoch 174/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2073\n",
            "Epoch 175/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2083\n",
            "Epoch 176/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2078\n",
            "Epoch 177/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2059\n",
            "Epoch 178/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2058\n",
            "Epoch 179/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2050\n",
            "Epoch 180/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2069\n",
            "Epoch 181/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2072\n",
            "Epoch 182/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2065\n",
            "Epoch 183/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2061\n",
            "Epoch 184/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2047\n",
            "Epoch 185/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2050\n",
            "Epoch 186/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2055\n",
            "Epoch 187/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2053\n",
            "Epoch 188/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2054\n",
            "Epoch 189/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2043\n",
            "Epoch 190/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2033\n",
            "Epoch 191/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2045\n",
            "Epoch 192/515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2036\n",
            "Epoch 193/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2034\n",
            "Epoch 194/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2022\n",
            "Epoch 195/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2036\n",
            "Epoch 196/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2031\n",
            "Epoch 197/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2030\n",
            "Epoch 198/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2035\n",
            "Epoch 199/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2029\n",
            "Epoch 200/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2033\n",
            "Epoch 201/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2037 0s - lo\n",
            "Epoch 202/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2041\n",
            "Epoch 203/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2013\n",
            "Epoch 204/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2011\n",
            "Epoch 205/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2027\n",
            "Epoch 206/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2014\n",
            "Epoch 207/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2004\n",
            "Epoch 208/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2009\n",
            "Epoch 209/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2020\n",
            "Epoch 210/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1998\n",
            "Epoch 211/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2006\n",
            "Epoch 212/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2011\n",
            "Epoch 213/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2009\n",
            "Epoch 214/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2019\n",
            "Epoch 215/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2020\n",
            "Epoch 216/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2012\n",
            "Epoch 217/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2002\n",
            "Epoch 218/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1998\n",
            "Epoch 219/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2003\n",
            "Epoch 220/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1988\n",
            "Epoch 221/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2011\n",
            "Epoch 222/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1998\n",
            "Epoch 223/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1995\n",
            "Epoch 224/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1998\n",
            "Epoch 225/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2001\n",
            "Epoch 226/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1987\n",
            "Epoch 227/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1999\n",
            "Epoch 228/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1977\n",
            "Epoch 229/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1988\n",
            "Epoch 230/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1974\n",
            "Epoch 231/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1993\n",
            "Epoch 232/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1977\n",
            "Epoch 233/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1996\n",
            "Epoch 234/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1975\n",
            "Epoch 235/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.2000\n",
            "Epoch 236/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1970\n",
            "Epoch 237/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1976\n",
            "Epoch 238/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1965\n",
            "Epoch 239/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1957\n",
            "Epoch 240/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1972\n",
            "Epoch 241/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1979\n",
            "Epoch 242/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1966\n",
            "Epoch 243/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1987\n",
            "Epoch 244/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1974\n",
            "Epoch 245/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1964\n",
            "Epoch 246/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1962\n",
            "Epoch 247/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1966\n",
            "Epoch 248/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1969\n",
            "Epoch 249/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1958\n",
            "Epoch 250/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1968\n",
            "Epoch 251/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1966\n",
            "Epoch 252/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1958\n",
            "Epoch 253/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1959\n",
            "Epoch 254/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1983\n",
            "Epoch 255/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1992\n",
            "Epoch 256/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1963\n",
            "Epoch 257/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1961\n",
            "Epoch 258/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1946\n",
            "Epoch 259/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1958\n",
            "Epoch 260/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1968\n",
            "Epoch 261/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1975\n",
            "Epoch 262/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1945\n",
            "Epoch 263/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1962\n",
            "Epoch 264/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1960\n",
            "Epoch 265/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1944\n",
            "Epoch 266/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1947\n",
            "Epoch 267/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1929\n",
            "Epoch 268/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1945\n",
            "Epoch 269/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1949\n",
            "Epoch 270/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1947\n",
            "Epoch 271/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1944\n",
            "Epoch 272/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1946\n",
            "Epoch 273/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1937\n",
            "Epoch 274/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1931\n",
            "Epoch 275/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1938\n",
            "Epoch 276/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1933\n",
            "Epoch 277/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1934\n",
            "Epoch 278/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1935\n",
            "Epoch 279/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1946\n",
            "Epoch 280/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1932\n",
            "Epoch 281/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1931\n",
            "Epoch 282/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1932\n",
            "Epoch 283/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1943\n",
            "Epoch 284/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1919\n",
            "Epoch 285/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1923\n",
            "Epoch 286/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1930\n",
            "Epoch 287/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1917\n",
            "Epoch 288/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1927\n",
            "Epoch 289/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1931\n",
            "Epoch 290/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1928\n",
            "Epoch 291/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1925\n",
            "Epoch 292/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1909\n",
            "Epoch 293/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1918\n",
            "Epoch 294/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1911\n",
            "Epoch 295/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1897\n",
            "Epoch 296/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1895\n",
            "Epoch 297/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1916\n",
            "Epoch 298/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1918\n",
            "Epoch 299/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1918\n",
            "Epoch 300/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1921\n",
            "Epoch 301/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1905\n",
            "Epoch 302/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1910\n",
            "Epoch 303/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1908\n",
            "Epoch 304/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1912\n",
            "Epoch 305/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1905\n",
            "Epoch 306/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1896\n",
            "Epoch 307/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1916\n",
            "Epoch 308/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1912\n",
            "Epoch 309/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1903 0s\n",
            "Epoch 310/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1921\n",
            "Epoch 311/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1904\n",
            "Epoch 312/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1887\n",
            "Epoch 313/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1898\n",
            "Epoch 314/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1900\n",
            "Epoch 315/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1894\n",
            "Epoch 316/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1892\n",
            "Epoch 317/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1903\n",
            "Epoch 318/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1890\n",
            "Epoch 319/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1905\n",
            "Epoch 320/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1895\n",
            "Epoch 321/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1912\n",
            "Epoch 322/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1904\n",
            "Epoch 323/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1892\n",
            "Epoch 324/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1894\n",
            "Epoch 325/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1890\n",
            "Epoch 326/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1891\n",
            "Epoch 327/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1879\n",
            "Epoch 328/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1877\n",
            "Epoch 329/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1874\n",
            "Epoch 330/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1891\n",
            "Epoch 331/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1888\n",
            "Epoch 332/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1885\n",
            "Epoch 333/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1866\n",
            "Epoch 334/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1874\n",
            "Epoch 335/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1869\n",
            "Epoch 336/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1879\n",
            "Epoch 337/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1882\n",
            "Epoch 338/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1884\n",
            "Epoch 339/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1889\n",
            "Epoch 340/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1896\n",
            "Epoch 341/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1890\n",
            "Epoch 342/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1883\n",
            "Epoch 343/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1895\n",
            "Epoch 344/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1891\n",
            "Epoch 345/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1869\n",
            "Epoch 346/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1861\n",
            "Epoch 347/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1875\n",
            "Epoch 348/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1870\n",
            "Epoch 349/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1880\n",
            "Epoch 350/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1871\n",
            "Epoch 351/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1865\n",
            "Epoch 352/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1880\n",
            "Epoch 353/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1858\n",
            "Epoch 354/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1848\n",
            "Epoch 355/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1860\n",
            "Epoch 356/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1855\n",
            "Epoch 357/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1850\n",
            "Epoch 358/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1862\n",
            "Epoch 359/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1861\n",
            "Epoch 360/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1867\n",
            "Epoch 361/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1860\n",
            "Epoch 362/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1856\n",
            "Epoch 363/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1918\n",
            "Epoch 364/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1876\n",
            "Epoch 365/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1859\n",
            "Epoch 366/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1858\n",
            "Epoch 367/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1856\n",
            "Epoch 368/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1841\n",
            "Epoch 369/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1867\n",
            "Epoch 370/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1850\n",
            "Epoch 371/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1853\n",
            "Epoch 372/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1848\n",
            "Epoch 373/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1858\n",
            "Epoch 374/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1836\n",
            "Epoch 375/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1861 0s - loss: 1\n",
            "Epoch 376/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1846\n",
            "Epoch 377/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1841\n",
            "Epoch 378/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1845\n",
            "Epoch 379/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1840\n",
            "Epoch 380/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1848\n",
            "Epoch 381/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1854\n",
            "Epoch 382/515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1839\n",
            "Epoch 383/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1850\n",
            "Epoch 384/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1852\n",
            "Epoch 385/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1829\n",
            "Epoch 386/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1821\n",
            "Epoch 387/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1838\n",
            "Epoch 388/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1831\n",
            "Epoch 389/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1842\n",
            "Epoch 390/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1844\n",
            "Epoch 391/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1829\n",
            "Epoch 392/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1837\n",
            "Epoch 393/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1824\n",
            "Epoch 394/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1829\n",
            "Epoch 395/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1831\n",
            "Epoch 396/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1855\n",
            "Epoch 397/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1828\n",
            "Epoch 398/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1819\n",
            "Epoch 399/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1820\n",
            "Epoch 400/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1837\n",
            "Epoch 401/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1823\n",
            "Epoch 402/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1862\n",
            "Epoch 403/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1820\n",
            "Epoch 404/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1831\n",
            "Epoch 405/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1835\n",
            "Epoch 406/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1829\n",
            "Epoch 407/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1835\n",
            "Epoch 408/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1828\n",
            "Epoch 409/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1813\n",
            "Epoch 410/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1820\n",
            "Epoch 411/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1825\n",
            "Epoch 412/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1804\n",
            "Epoch 413/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1810\n",
            "Epoch 414/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1828\n",
            "Epoch 415/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1831\n",
            "Epoch 416/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1834\n",
            "Epoch 417/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1829\n",
            "Epoch 418/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1810\n",
            "Epoch 419/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1825\n",
            "Epoch 420/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1821\n",
            "Epoch 421/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1814\n",
            "Epoch 422/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1824\n",
            "Epoch 423/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1825\n",
            "Epoch 424/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1799\n",
            "Epoch 425/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1815\n",
            "Epoch 426/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1835\n",
            "Epoch 427/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1823\n",
            "Epoch 428/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1822\n",
            "Epoch 429/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1813\n",
            "Epoch 430/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1822\n",
            "Epoch 431/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1818\n",
            "Epoch 432/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1827\n",
            "Epoch 433/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1810\n",
            "Epoch 434/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1815\n",
            "Epoch 435/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1816\n",
            "Epoch 436/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1820\n",
            "Epoch 437/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1798\n",
            "Epoch 438/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1808\n",
            "Epoch 439/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1804\n",
            "Epoch 440/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1811\n",
            "Epoch 441/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1795\n",
            "Epoch 442/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1801\n",
            "Epoch 443/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1799\n",
            "Epoch 444/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1815\n",
            "Epoch 445/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1803\n",
            "Epoch 446/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1792\n",
            "Epoch 447/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1806\n",
            "Epoch 448/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1817\n",
            "Epoch 449/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1795\n",
            "Epoch 450/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1798\n",
            "Epoch 451/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1810\n",
            "Epoch 452/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1809\n",
            "Epoch 453/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1800\n",
            "Epoch 454/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1797\n",
            "Epoch 455/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1795\n",
            "Epoch 456/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1804\n",
            "Epoch 457/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1795\n",
            "Epoch 458/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1809\n",
            "Epoch 459/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1796\n",
            "Epoch 460/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1802\n",
            "Epoch 461/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1784\n",
            "Epoch 462/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1809\n",
            "Epoch 463/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1797\n",
            "Epoch 464/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1791\n",
            "Epoch 465/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1789\n",
            "Epoch 466/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1787\n",
            "Epoch 467/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1791\n",
            "Epoch 468/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1798\n",
            "Epoch 469/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1785\n",
            "Epoch 470/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1788\n",
            "Epoch 471/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1794\n",
            "Epoch 472/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1796\n",
            "Epoch 473/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1792\n",
            "Epoch 474/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1777\n",
            "Epoch 475/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1786\n",
            "Epoch 476/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1797\n",
            "Epoch 477/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1777\n",
            "Epoch 478/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1772\n",
            "Epoch 479/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1805\n",
            "Epoch 480/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1782\n",
            "Epoch 481/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1787\n",
            "Epoch 482/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1796\n",
            "Epoch 483/515\n",
            "288/288 [==============================] - 10s 33ms/step - loss: 1.1781\n",
            "Epoch 484/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1783\n",
            "Epoch 485/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1782\n",
            "Epoch 486/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1780\n",
            "Epoch 487/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1777\n",
            "Epoch 488/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1772\n",
            "Epoch 489/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1777\n",
            "Epoch 490/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1785\n",
            "Epoch 491/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1770\n",
            "Epoch 492/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1779\n",
            "Epoch 493/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1784\n",
            "Epoch 494/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1782\n",
            "Epoch 495/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1786\n",
            "Epoch 496/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1767\n",
            "Epoch 497/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1763\n",
            "Epoch 498/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1763\n",
            "Epoch 499/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1770\n",
            "Epoch 500/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1770\n",
            "Epoch 501/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1767\n",
            "Epoch 502/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1769\n",
            "Epoch 503/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1764\n",
            "Epoch 504/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1765\n",
            "Epoch 505/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1769\n",
            "Epoch 506/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1762\n",
            "Epoch 507/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1764\n",
            "Epoch 508/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1756\n",
            "Epoch 509/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1763\n",
            "Epoch 510/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1755\n",
            "Epoch 511/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1771\n",
            "Epoch 512/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1759\n",
            "Epoch 513/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1769\n",
            "Epoch 514/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1758\n",
            "Epoch 515/515\n",
            "288/288 [==============================] - 10s 34ms/step - loss: 1.1771\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "execution_count": 38,
      "metadata": {
        "scrolled": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment=Experiment(ws, 'buffy')\n",
        "\n",
        "#Return all runs\n",
        "from tqdm import tqdm\n",
        "runs = {}\n",
        "run_metrics = {}\n",
        "runsorder = {}\n",
        "i = 0\n",
        "\n",
        "for r in tqdm(experiment.get_runs()):\n",
        "    metrics = r.get_metrics()\n",
        "    if 'epochs' in metrics.keys():\n",
        "        i = i + 1\n",
        "        runs[r.id] = r\n",
        "        run_metrics[r.id] = metrics"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "6it [00:05,  1.04it/s]\n"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(run_metrics.values())"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epochs</th>\n      <th>LSTM</th>\n      <th>Min Loss</th>\n      <th>loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>515</td>\n      <td>64</td>\n      <td>1.175454</td>\n      <td>aml://artifactId/ExperimentRun/dcid.861e544e-1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>500</td>\n      <td>64</td>\n      <td>0.983079</td>\n      <td>aml://artifactId/ExperimentRun/dcid.3333d8e5-4...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>200</td>\n      <td>64</td>\n      <td>1.038818</td>\n      <td>aml://artifactId/ExperimentRun/dcid.28b29c80-3...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100</td>\n      <td>64</td>\n      <td>1.115816</td>\n      <td>aml://artifactId/ExperimentRun/dcid.256cde82-6...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20</td>\n      <td>64</td>\n      <td>1.381550</td>\n      <td>aml://artifactId/ExperimentRun/dcid.fa458fdd-5...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>10</td>\n      <td>64</td>\n      <td>1.537578</td>\n      <td>aml://artifactId/ExperimentRun/dcid.871aa7ff-3...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   epochs  LSTM  Min Loss                                               loss\n0     515    64  1.175454  aml://artifactId/ExperimentRun/dcid.861e544e-1...\n1     500    64  0.983079  aml://artifactId/ExperimentRun/dcid.3333d8e5-4...\n2     200    64  1.038818  aml://artifactId/ExperimentRun/dcid.28b29c80-3...\n3     100    64  1.115816  aml://artifactId/ExperimentRun/dcid.256cde82-6...\n4      20    64  1.381550  aml://artifactId/ExperimentRun/dcid.fa458fdd-5...\n5      10    64  1.537578  aml://artifactId/ExperimentRun/dcid.871aa7ff-3..."
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "allruns = pd.DataFrame.from_dict(run_metrics, orient ='index')\n",
        "allruns"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epochs</th>\n      <th>LSTM</th>\n      <th>Min Loss</th>\n      <th>loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>861e544e-186a-484d-8a7e-2456372857d0</th>\n      <td>515</td>\n      <td>64</td>\n      <td>1.175454</td>\n      <td>aml://artifactId/ExperimentRun/dcid.861e544e-1...</td>\n    </tr>\n    <tr>\n      <th>3333d8e5-441b-40f2-9134-43e92ee36ea0</th>\n      <td>500</td>\n      <td>64</td>\n      <td>0.983079</td>\n      <td>aml://artifactId/ExperimentRun/dcid.3333d8e5-4...</td>\n    </tr>\n    <tr>\n      <th>28b29c80-359b-4c70-9591-b7e8ae25e2f4</th>\n      <td>200</td>\n      <td>64</td>\n      <td>1.038818</td>\n      <td>aml://artifactId/ExperimentRun/dcid.28b29c80-3...</td>\n    </tr>\n    <tr>\n      <th>256cde82-649e-4e75-bb63-6d91e3cb3dc0</th>\n      <td>100</td>\n      <td>64</td>\n      <td>1.115816</td>\n      <td>aml://artifactId/ExperimentRun/dcid.256cde82-6...</td>\n    </tr>\n    <tr>\n      <th>fa458fdd-56a5-4f5c-9914-949df8536ee8</th>\n      <td>20</td>\n      <td>64</td>\n      <td>1.381550</td>\n      <td>aml://artifactId/ExperimentRun/dcid.fa458fdd-5...</td>\n    </tr>\n    <tr>\n      <th>871aa7ff-3aa4-4dfd-a1e7-f1527c4d07a7</th>\n      <td>10</td>\n      <td>64</td>\n      <td>1.537578</td>\n      <td>aml://artifactId/ExperimentRun/dcid.871aa7ff-3...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                                      epochs  LSTM  Min Loss  \\\n861e544e-186a-484d-8a7e-2456372857d0     515    64  1.175454   \n3333d8e5-441b-40f2-9134-43e92ee36ea0     500    64  0.983079   \n28b29c80-359b-4c70-9591-b7e8ae25e2f4     200    64  1.038818   \n256cde82-649e-4e75-bb63-6d91e3cb3dc0     100    64  1.115816   \nfa458fdd-56a5-4f5c-9914-949df8536ee8      20    64  1.381550   \n871aa7ff-3aa4-4dfd-a1e7-f1527c4d07a7      10    64  1.537578   \n\n                                                                                   loss  \n861e544e-186a-484d-8a7e-2456372857d0  aml://artifactId/ExperimentRun/dcid.861e544e-1...  \n3333d8e5-441b-40f2-9134-43e92ee36ea0  aml://artifactId/ExperimentRun/dcid.3333d8e5-4...  \n28b29c80-359b-4c70-9591-b7e8ae25e2f4  aml://artifactId/ExperimentRun/dcid.28b29c80-3...  \n256cde82-649e-4e75-bb63-6d91e3cb3dc0  aml://artifactId/ExperimentRun/dcid.256cde82-6...  \nfa458fdd-56a5-4f5c-9914-949df8536ee8  aml://artifactId/ExperimentRun/dcid.fa458fdd-5...  \n871aa7ff-3aa4-4dfd-a1e7-f1527c4d07a7  aml://artifactId/ExperimentRun/dcid.871aa7ff-3...  "
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the run\n",
        "epoch = 515\n",
        "\n",
        "run_by_epoch = runs[allruns[allruns['epochs'] == epoch].index[0]]\n",
        "\n",
        "#Get the model\n",
        "for f in range(len(run_by_epoch.get_file_names())):\n",
        "    if (run_by_epoch.get_file_names()[f] == 'outputs/predmodel.h5'):\n",
        "        model_path = run_by_epoch.get_file_names()[f]\n",
        "model_path\n",
        "\n",
        "#Download model\n",
        "res = run_by_epoch.download_file(model_path)\n",
        "#Load model\n",
        "model = keras.models.load_model('predmodel.h5')\n"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length = 800 # Amount of text to create\n",
        "diversity = 0.2\n",
        "\n",
        "generated = ''\n",
        "\n",
        "start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "sentence = text[start_index: start_index + maxlen]\n",
        "\n",
        "sentence = \"buffy: lets go kill some vampires faith \"\n",
        "\n",
        "print(sentence)\n",
        "\n",
        "for i in range(length):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        \n",
        "        preds = np.asarray(preds).astype('float64')\n",
        "        preds = np.log(preds) / diversity\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        probas = np.random.multinomial(1, preds, 1)\n",
        "        \n",
        "        next_index = np.argmax(probas)\n",
        "        next_char = indices_char[next_index]\n",
        "\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "\n",
        "\n",
        "print(generated)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "buffy: lets go kill some vampires faith \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning: divide by zero encountered in log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "that is a stake and then she starts to be the stacks. \n",
            "xander: well, i don't know and start to the books. \n",
            "buffy: well, i don't know and start that i don't know it work. \n",
            "buffy: well, i don't know what you know what you know he does a little good. \n",
            "buffy: (to buffy) i don't know her was the master. \n",
            "buffy: what's the master? \n",
            "buffy: i don't know and goes and there's a student of the stake and starts to be the bronze. \n",
            "buffy: i don't know what you should go on the master? \n",
            "buffy: what's the hall to the master? \n",
            "buffy: (grabs his hand) that's a stake. \n",
            "xander: what's the hall? \n",
            "buffy: okay, there's a student. \n",
            "buffy: i don't know what you know what you don't know what i don't know what you know it was a stake. \n",
            "xander: that's the hall of the way. \n",
            "buffy: well, i don't know what you know it's\n"
          ]
        }
      ],
      "execution_count": 20,
      "metadata": {
        "scrolled": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#plt.plot(history.history['loss'])\n",
        "#plt.title('model loss')\n",
        "#plt.ylabel('loss')\n",
        "#plt.xlabel('epoch')\n",
        "#plt.legend(['train', 'validation'], loc='upper left')\n",
        "#plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, diversity=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / diversity\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the model to disk\n",
        "model.save('model200')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the model\n",
        "model = keras.models.load_model('model200')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(length, diversity):\n",
        "    # Get random starting text\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    for i in range(length):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "    return generated"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(500, 0.1))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": [
        "buffy: lets go kill some vampires faith \n",
        "and they all see the hall. \n",
        "buffy: (to his hand and stands to her and starts to her face. \n",
        "cordelia: i don't see the stairs. \n",
        "buffy: (to her back around) \n",
        "buffy: (to her and gets up) \n",
        "giles: (starts to her friend) i want to see the stairs. \n",
        "buffy: (starts to get up) \n",
        "buffy: (stands to giles) i don't see the stairs. \n",
        "buffy: (starts to get out) \n",
        "cordelia: (starts to buffy) \n",
        "buffy: (starts to get out) \n",
        "buffy: i don't think it's a bar. \n",
        "buffy: (stands to her and starts to buffy and stands to the bac"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": [
        "r: well, he's buff! she never said anything and then but hes to her stake. \n",
        "willow: oh, he's not like i won't have to to be fun. \n",
        "she gives her a look at her and sees night... \n",
        "willow: well, whe don't we say that you want me? \n",
        "willow stares up. \n",
        "xander: well, whe do we saw him as finds of little \n",
        "a few school. \n",
        "xander: what do we don't know. \n",
        "willow: oh, he's so front. \n",
        "willow: i think it was the she slayer, see that she's still be stuff... \n",
        "buffy: well, it's a little in the staff fighter. \n",
        "willow: i think it's the feners cordelia's c"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}